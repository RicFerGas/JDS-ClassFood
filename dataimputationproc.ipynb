{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from miceforest import ImputationKernel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "df = pd.read_csv('food.csv', sep='\\t')\n",
    "# we only select rows where food_groups_en is not null\n",
    "df = df.dropna(subset=['food_groups_en'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low correlated column dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlations for categorical data\n",
    "columns = list(df.select_dtypes(include=['object']).columns)\n",
    "corr = pd.Series(index = columns)\n",
    "\n",
    "for j in columns : \n",
    "    try :\n",
    "        corr.loc[j] = cramers_v(df[j],df['food_groups_en'])\n",
    "    except :\n",
    "        print('Error, we put correlation = 0 for the column  :  ' + str(j))\n",
    "        corr.loc[j] = 0\n",
    "\n",
    "\n",
    "#numerical_columns = list(corr_numerical[abs(corr_numerical)>= 0.1].index)\n",
    "#numerical_columns.remove('food_groups_en')\n",
    "corr_categorical = corr.sort_values(ascending=False)\n",
    "removed_columns=  list(corr_categorical[abs(corr_categorical)<= 0.2].index)\n",
    "categorical_columns = list(corr_categorical[abs(corr_categorical)>= 0.2].index)\n",
    "\n",
    "# we drop columns that contains informations about the category of the product, because we want to prevent from data leakage\n",
    "# we are not supposed to have theses columns at the time of the prediction\n",
    "\n",
    "columns_to_keep = categorical_columns\n",
    "columns_to_delete = ['food_groups_tags','pnns_groups_1','pnns_groups_2','categories_en','categories_tags', 'main_category_en', 'main_category', 'categories']\n",
    "\n",
    "removed_columns.extend([i for i in columns_to_delete if i not in removed_columns])\n",
    "filtered_drop_data = df.drop(columns=removed_columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values column dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of missing data in each column\n",
    "missing_data = filtered_drop_data.isnull().mean()\n",
    "\n",
    "# Set a threshold for the proportion of missing data (e.g., 70%)\n",
    "threshold = 0.7\n",
    "\n",
    "# Filter columns that meet the threshold criteria\n",
    "filtered_columns = missing_data[missing_data <= threshold].index\n",
    "\n",
    "# Create a new DataFrame with the filtered columns\n",
    "filtered_data_mv = filtered_drop_data[filtered_columns]\n",
    "\n",
    "# Remove additional columns that are not needed\n",
    "filtered_data_mv = filtered_data_mv.drop(columns=[\"last_modified_datetime\", \"created_datetime\", \"created_t\", \"last_modified_t\", \"last_image_t\", \"last_image_t\"])\n",
    "filtered_data_mvnum=filtered_data_mv.dropna(subset=['sodium_100g',\"nutrient_levels_tags\",\"ingredients_text\"])\n",
    "filtered_data_mvnum=filtered_data_mv.dropna(subset=['proteins_100g',\"nutriscore_grade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP LINES BASED ON THE TRESHOLD \n",
    "# Create a list of column names to drop based on the threshold\n",
    "treshold_2= 0.5\n",
    "columns_to_drop = [col for col in filtered_data_mvnum.columns if filtered_data_mvnum[col].isna().mean() > treshold_2]\n",
    "\n",
    "# Drop the selected columns from the DataFrame\n",
    "filtered_data_mvnum = filtered_data_mvnum.drop(columns=columns_to_drop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value for each numerical column in the filtered data\n",
    "mean_value = filtered_data_mvnum.mean()\n",
    "\n",
    "# Perform mean imputation by filling missing values with column means\n",
    "mean_imputation = filtered_data_mvnum.select_dtypes(include=['number']).fillna(mean_value)\n",
    "\n",
    "# Set a threshold (60%) for the number of columns with NaN within the same 'food_groups' category\n",
    "\n",
    "# Group the DataFrame by the 'food_groups' column\n",
    "grouped = filtered_data_mvnum.groupby('food_groups')\n",
    "\n",
    "# Create a list to store the corrected groups\n",
    "filled_groups = []\n",
    "\n",
    "# Iterate through each group and fill NaN values with 0 if more than 60% of columns in that group have NaN\n",
    "for name, group in grouped:\n",
    "    # Calculate the proportion of missing values in each column of the group\n",
    "    missing_data = group.isnull().mean()\n",
    "    \n",
    "    # Filter numerical columns with more than 60% NaN\n",
    "    columns_to_fill_with_zero = missing_data[missing_data > threshold].index\n",
    "    columns_to_fill_with_mean = missing_data[missing_data <= threshold].index\n",
    "\n",
    "    # Fill missing values with 0 in the selected columns\n",
    "    group[columns_to_fill_with_zero] = group[columns_to_fill_with_zero].fillna(0)\n",
    "    \n",
    "    # Fill missing values with the mean in the selected numerical columns\n",
    "    for column in columns_to_fill_with_mean:\n",
    "        if pd.api.types.is_numeric_dtype(group[column]):\n",
    "            group[column] = group[column].fillna(group[column].mean())\n",
    "\n",
    "    # Add the corrected group to the list\n",
    "    filled_groups.append(group)\n",
    "\n",
    "# Concatenate the corrected groups into a single DataFrame\n",
    "imputed_data = pd.concat(filled_groups)\n",
    "\n",
    "# Reset the indices for the resulting DataFrame\n",
    "imputed_data = imputed_data.reset_index(drop=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
